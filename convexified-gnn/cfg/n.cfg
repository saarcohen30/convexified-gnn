[DEFAULT]

alg = dagger

# learning parameters
batch_size = 20
buffer_size = 5000
updates_per_step = 200
seed = 12
actor_lr = 5e-5

n_train_episodes = 400
beta_coeff = 0.993
test_interval = 40
n_test_episodes = 20

# architecture parameters
k = 2
hidden_size = 32
gamma = 0.99
tau = 0.5

# env parameters
env = FlockingRelative-v0
v_max = 3.0
comm_radius = 1.0
n_agents = 100
n_actions = 2
n_states = 6
debug = False
dt = 0.01


header = k, n_agents, reward

[4, 150]
k = 4
n_agents = 150

[1, 25]
k = 1
n_agents = 25

[1, 50]
k = 1
n_agents = 50

[1, 75]
k = 1
n_agents = 75

[1, 100]
k = 1
n_agents = 100

[1, 125]
k = 1
n_agents = 125

[1, 150]
k = 1
n_agents = 150


[2, 25]
k = 2
n_agents = 25

[2, 50]
k = 2
n_agents = 50

[2, 75]
k = 2
n_agents = 75

[2, 100]
k = 2
n_agents = 100

[2, 125]
k = 2
n_agents = 125

[2, 150]
k = 2
n_agents = 150

[3, 25]
k = 3
n_agents = 25

[3, 50]
k = 3
n_agents = 50

[3, 75]
k = 3
n_agents = 75

[3, 100]
k = 3
n_agents = 100

[3, 125]
k = 3
n_agents = 125

[3, 150]
k = 3
n_agents = 150

[4, 25]
k = 4
n_agents = 25

[4, 50]
k = 4
n_agents = 50

[4, 75]
k = 4
n_agents = 75

[4, 100]
k = 4
n_agents = 100

[4, 125]
k = 4
n_agents = 125




